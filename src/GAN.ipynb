{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-19T19:09:05.737524Z",
     "iopub.status.busy": "2021-12-19T19:09:05.736556Z",
     "iopub.status.idle": "2021-12-19T19:09:10.738707Z",
     "shell.execute_reply": "2021-12-19T19:09:10.738082Z",
     "shell.execute_reply.started": "2021-12-19T19:09:05.737465Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        break\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:09:10.746525Z",
     "iopub.status.busy": "2021-12-19T19:09:10.746072Z",
     "iopub.status.idle": "2021-12-19T19:09:10.785104Z",
     "shell.execute_reply": "2021-12-19T19:09:10.784586Z",
     "shell.execute_reply.started": "2021-12-19T19:09:10.746492Z"
    }
   },
   "outputs": [],
   "source": [
    "# artist_names = ['Edgar Degas']\n",
    "num_artists = 4\n",
    "\n",
    "artist_df = pd.read_csv('/kaggle/input/best-artworks-of-all-time/artists.csv')\n",
    "artist_df = artist_df.sort_values(by='paintings', ascending=False)\n",
    "artist_df = artist_df.iloc[:num_artists]\n",
    "# artist_df = artist_df[artist_df['name'].isin(artist_names)]\n",
    "artist_list = artist_df['name'].str.replace(' ', '_').tolist()\n",
    "artist_dict = {}\n",
    "for i, n in enumerate(artist_list):\n",
    "    artist_dict[n] = i\n",
    "    \n",
    "artist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:09:10.786688Z",
     "iopub.status.busy": "2021-12-19T19:09:10.786008Z",
     "iopub.status.idle": "2021-12-19T19:09:10.803804Z",
     "shell.execute_reply": "2021-12-19T19:09:10.803202Z",
     "shell.execute_reply.started": "2021-12-19T19:09:10.786658Z"
    }
   },
   "outputs": [],
   "source": [
    "artist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:09:10.806126Z",
     "iopub.status.busy": "2021-12-19T19:09:10.805488Z",
     "iopub.status.idle": "2021-12-19T19:10:45.587054Z",
     "shell.execute_reply": "2021-12-19T19:10:45.586032Z",
     "shell.execute_reply.started": "2021-12-19T19:09:10.806090Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_dir = '/kaggle/output/preprocessed'\n",
    "image_dir = '/kaggle/input/best-artworks-of-all-time/images/images'\n",
    "\n",
    "def preprocess(artist_name, image_size):\n",
    "    \"\"\"\n",
    "    preprocess images into new image size and save them into corresponding dirs\n",
    "    \"\"\"\n",
    "    artist_image_dir = os.path.join(image_dir, artist_name)\n",
    "    artist_preprocessed_dir = os.path.join(preprocessed_dir, artist_name)\n",
    "\n",
    "    !rm -rf {artist_preprocessed_dir}\n",
    "    if not os.path.exists(artist_preprocessed_dir):\n",
    "        os.makedirs(artist_preprocessed_dir)\n",
    "\n",
    "    image_list = []\n",
    "    for dirname, _, filenames in os.walk(artist_image_dir):\n",
    "        for filename in filenames:\n",
    "            image = Image.open(os.path.join(dirname, filename)).convert('RGB')\n",
    "            image = image.resize(image_size)\n",
    "            image_list.append(np.asarray(image))\n",
    "            # image.save(os.path.join(artist_preprocessed_dir, filename))\n",
    "    return image_list\n",
    "\n",
    "\n",
    "image_size = (128, 128)\n",
    "\n",
    "image_list = []\n",
    "label_list = []\n",
    "for artist_name in artist_list:\n",
    "    il = preprocess(artist_name, image_size)\n",
    "    image_list += il\n",
    "    label_list += [artist_dict[artist_name] for i in range(len(il))]\n",
    "    \n",
    "image_list = np.array(image_list)\n",
    "image_list = (image_list - 127.5) / 127.5\n",
    "label_list = np.array(label_list)\n",
    "assert len(image_list) == len(label_list)\n",
    "print(len(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:45.589183Z",
     "iopub.status.busy": "2021-12-19T19:10:45.588847Z",
     "iopub.status.idle": "2021-12-19T19:10:47.364393Z",
     "shell.execute_reply": "2021-12-19T19:10:47.363615Z",
     "shell.execute_reply.started": "2021-12-19T19:10:45.589139Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_artworks(image_list):\n",
    "    \"\"\"\n",
    "    display artworks in 6*6 format\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    for i in range(1,37):\n",
    "        fig.add_subplot(6,6,i)\n",
    "        plt.imshow(image_list[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "print(len(image_list))\n",
    "display_artworks(image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:47.366351Z",
     "iopub.status.busy": "2021-12-19T19:10:47.365643Z",
     "iopub.status.idle": "2021-12-19T19:10:47.805740Z",
     "shell.execute_reply": "2021-12-19T19:10:47.805078Z",
     "shell.execute_reply.started": "2021-12-19T19:10:47.366313Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from IPython import display\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:47.807452Z",
     "iopub.status.busy": "2021-12-19T19:10:47.807043Z",
     "iopub.status.idle": "2021-12-19T19:10:48.688644Z",
     "shell.execute_reply": "2021-12-19T19:10:48.687913Z",
     "shell.execute_reply.started": "2021-12-19T19:10:47.807403Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images = image_list\n",
    "train_labels = label_list\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:48.690868Z",
     "iopub.status.busy": "2021-12-19T19:10:48.690377Z",
     "iopub.status.idle": "2021-12-19T19:10:48.702272Z",
     "shell.execute_reply": "2021-12-19T19:10:48.701371Z",
     "shell.execute_reply.started": "2021-12-19T19:10:48.690818Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Conv2D, Conv2DTranspose, Reshape, Flatten\n",
    "from keras.layers import Dropout, LeakyReLU, BatchNormalization\n",
    "from keras.layers import Activation, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers import Input, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:48.704242Z",
     "iopub.status.busy": "2021-12-19T19:10:48.703803Z",
     "iopub.status.idle": "2021-12-19T19:10:48.718321Z",
     "shell.execute_reply": "2021-12-19T19:10:48.717198Z",
     "shell.execute_reply.started": "2021-12-19T19:10:48.704193Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    \"\"\"\n",
    "    definition of generator model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128*128,activation=\"relu\",input_dim=100)) #128x128 units\n",
    "    model.add(Reshape((8,8,256)))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "   \n",
    "    model.add(UpSampling2D(size=(2,2)))\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(UpSampling2D(size=(2,2)))\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Conv2D(3,kernel_size=3,padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:48.736798Z",
     "iopub.status.busy": "2021-12-19T19:10:48.735873Z",
     "iopub.status.idle": "2021-12-19T19:10:48.749149Z",
     "shell.execute_reply": "2021-12-19T19:10:48.748441Z",
     "shell.execute_reply.started": "2021-12-19T19:10:48.736752Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    \"\"\"\n",
    "    definition of discriminator model\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[128, 128, 3]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:48.751317Z",
     "iopub.status.busy": "2021-12-19T19:10:48.750207Z",
     "iopub.status.idle": "2021-12-19T19:10:49.655636Z",
     "shell.execute_reply": "2021-12-19T19:10:49.654771Z",
     "shell.execute_reply.started": "2021-12-19T19:10:48.751269Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "# random noise for generation\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0,:,:,0],interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:49.658693Z",
     "iopub.status.busy": "2021-12-19T19:10:49.658474Z",
     "iopub.status.idle": "2021-12-19T19:10:49.844843Z",
     "shell.execute_reply": "2021-12-19T19:10:49.843873Z",
     "shell.execute_reply.started": "2021-12-19T19:10:49.658666Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:49.846348Z",
     "iopub.status.busy": "2021-12-19T19:10:49.846079Z",
     "iopub.status.idle": "2021-12-19T19:10:49.853642Z",
     "shell.execute_reply": "2021-12-19T19:10:49.852704Z",
     "shell.execute_reply.started": "2021-12-19T19:10:49.846320Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    discriminator loss function of cross entropy\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    \"\"\"\n",
    "    generator loss function of cross entropy\n",
    "    \"\"\"\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4,0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:49.855760Z",
     "iopub.status.busy": "2021-12-19T19:10:49.855440Z",
     "iopub.status.idle": "2021-12-19T19:10:52.238731Z",
     "shell.execute_reply": "2021-12-19T19:10:52.237851Z",
     "shell.execute_reply.started": "2021-12-19T19:10:49.855704Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf ./training_checkpoints\n",
    "!mkdir ./training_checkpoints\n",
    "!rm -rf ./image_at_epoch*\n",
    "\n",
    "\n",
    "# checkpoints for model trianing, save models every certain epochs\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:52.240245Z",
     "iopub.status.busy": "2021-12-19T19:10:52.240005Z",
     "iopub.status.idle": "2021-12-19T19:10:52.246423Z",
     "shell.execute_reply": "2021-12-19T19:10:52.245465Z",
     "shell.execute_reply.started": "2021-12-19T19:10:52.240219Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 4\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:52.247932Z",
     "iopub.status.busy": "2021-12-19T19:10:52.247599Z",
     "iopub.status.idle": "2021-12-19T19:10:52.260165Z",
     "shell.execute_reply": "2021-12-19T19:10:52.259526Z",
     "shell.execute_reply.started": "2021-12-19T19:10:52.247897Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    \"\"\"\n",
    "    train step for loss calculation and generation\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:52.262085Z",
     "iopub.status.busy": "2021-12-19T19:10:52.261232Z",
     "iopub.status.idle": "2021-12-19T19:10:52.276332Z",
     "shell.execute_reply": "2021-12-19T19:10:52.275435Z",
     "shell.execute_reply.started": "2021-12-19T19:10:52.262049Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    \"\"\"\n",
    "    generate, display, and save images\n",
    "    \"\"\"\n",
    "    predictions = model(test_input, training=False)\n",
    "    predictions = 0.5 * predictions + 0.5\n",
    "\n",
    "    fig = plt.figure(figsize=(32, 32))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T19:10:52.277857Z",
     "iopub.status.busy": "2021-12-19T19:10:52.277506Z",
     "iopub.status.idle": "2021-12-19T19:10:52.288927Z",
     "shell.execute_reply": "2021-12-19T19:10:52.287917Z",
     "shell.execute_reply.started": "2021-12-19T19:10:52.277826Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      t = train_step(image_batch)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    \n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-19T20:35:35.281651Z",
     "iopub.status.busy": "2021-12-19T20:35:35.281112Z"
    }
   },
   "outputs": [],
   "source": [
    "train(train_dataset, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-19T20:35:20.715767Z",
     "iopub.status.idle": "2021-12-19T20:35:20.716270Z",
     "shell.execute_reply": "2021-12-19T20:35:20.716116Z",
     "shell.execute_reply.started": "2021-12-19T20:35:20.716076Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = tf.random.normal([20, noise_dim])\n",
    "predictions = generator(seed, training=False)\n",
    "predictions = 0.5 * predictions + 0.5\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(1,21):\n",
    "    fig.add_subplot(5,5,i)\n",
    "    plt.imshow(predictions[i])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./generated_images\n",
    "!mkdir ./generated_images\n",
    "\n",
    "seed = tf.random.normal([200, noise_dim])\n",
    "predictions = generator(seed, training=False)\n",
    "predictions = 0.5 * predictions + 0.5\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(1,200):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(generated_images2[i])\n",
    "    plt.savefig('./images/%d.jpg' % i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
