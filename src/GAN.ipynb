{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-19T19:09:05.736556Z","iopub.execute_input":"2021-12-19T19:09:05.737524Z","iopub.status.idle":"2021-12-19T19:09:10.738707Z","shell.execute_reply.started":"2021-12-19T19:09:05.737465Z","shell.execute_reply":"2021-12-19T19:09:10.738082Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocess","metadata":{}},{"cell_type":"code","source":"# artist_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:09:10.740447Z","iopub.execute_input":"2021-12-19T19:09:10.740671Z","iopub.status.idle":"2021-12-19T19:09:10.744680Z","shell.execute_reply.started":"2021-12-19T19:09:10.740643Z","shell.execute_reply":"2021-12-19T19:09:10.743952Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# artist_names = ['Edgar Degas']\n\nartist_df = pd.read_csv('/kaggle/input/best-artworks-of-all-time/artists.csv')\nartist_df = artist_df.sort_values(by='paintings', ascending=False)\nartist_df = artist_df.iloc[:5]\n# artist_df = artist_df[artist_df['name'].isin(artist_names)]\nartist_list = artist_df['name'].str.replace(' ', '_').tolist()\nartist_dict = {}\nfor i, n in enumerate(artist_list):\n    artist_dict[n] = i\n    \nartist_dict","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:09:10.746072Z","iopub.execute_input":"2021-12-19T19:09:10.746525Z","iopub.status.idle":"2021-12-19T19:09:10.785104Z","shell.execute_reply.started":"2021-12-19T19:09:10.746492Z","shell.execute_reply":"2021-12-19T19:09:10.784586Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"artist_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:09:10.786008Z","iopub.execute_input":"2021-12-19T19:09:10.786688Z","iopub.status.idle":"2021-12-19T19:09:10.803804Z","shell.execute_reply.started":"2021-12-19T19:09:10.786658Z","shell.execute_reply":"2021-12-19T19:09:10.803202Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"preprocessed_dir = '/kaggle/output/preprocessed'\nimage_dir = '/kaggle/input/best-artworks-of-all-time/images/images'\n\ndef preprocess(artist_name, image_size):\n    artist_image_dir = os.path.join(image_dir, artist_name)\n    artist_preprocessed_dir = os.path.join(preprocessed_dir, artist_name)\n\n    !rm -rf {artist_preprocessed_dir}\n    if not os.path.exists(artist_preprocessed_dir):\n        os.makedirs(artist_preprocessed_dir)\n\n    image_list = []\n    for dirname, _, filenames in os.walk(artist_image_dir):\n        for filename in filenames:\n            image = Image.open(os.path.join(dirname, filename)).convert('RGB')\n            image = image.resize(image_size)\n            image_list.append(np.asarray(image))\n            # image.save(os.path.join(artist_preprocessed_dir, filename))\n    return image_list\n\n\nimage_size = (128, 128)\n\nimage_list = []\nlabel_list = []\nfor artist_name in artist_list:\n    il = preprocess(artist_name, image_size)\n    image_list += il\n    label_list += [artist_dict[artist_name] for i in range(len(il))]\n    \nimage_list = np.array(image_list)\nimage_list = (image_list - 127.5) / 127.5\nlabel_list = np.array(label_list)\nassert len(image_list) == len(label_list)\nprint(len(image_list))","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:09:10.805488Z","iopub.execute_input":"2021-12-19T19:09:10.806126Z","iopub.status.idle":"2021-12-19T19:10:45.587054Z","shell.execute_reply.started":"2021-12-19T19:09:10.806090Z","shell.execute_reply":"2021-12-19T19:10:45.586032Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def display_artworks(image_list):\n    fig = plt.figure(figsize=(8,8))\n    for i in range(1,37):\n        fig.add_subplot(6,6,i)\n        plt.imshow(image_list[i])\n        plt.axis('off')\n\nprint(len(image_list))\ndisplay_artworks(image_list)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:45.588847Z","iopub.execute_input":"2021-12-19T19:10:45.589183Z","iopub.status.idle":"2021-12-19T19:10:47.364393Z","shell.execute_reply.started":"2021-12-19T19:10:45.589139Z","shell.execute_reply":"2021-12-19T19:10:47.363615Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Dataset","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nimport time\nfrom IPython import display\nimport tensorflow as tf\ntf.__version__","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:47.365643Z","iopub.execute_input":"2021-12-19T19:10:47.366351Z","iopub.status.idle":"2021-12-19T19:10:47.805740Z","shell.execute_reply.started":"2021-12-19T19:10:47.366313Z","shell.execute_reply":"2021-12-19T19:10:47.805078Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_images = image_list\ntrain_labels = label_list\n\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:47.807043Z","iopub.execute_input":"2021-12-19T19:10:47.807452Z","iopub.status.idle":"2021-12-19T19:10:48.688644Z","shell.execute_reply.started":"2021-12-19T19:10:47.807403Z","shell.execute_reply":"2021-12-19T19:10:48.687913Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Define Models","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, Conv2D, Conv2DTranspose, Reshape, Flatten\nfrom keras.layers import Dropout, LeakyReLU, BatchNormalization\nfrom keras.layers import Activation, ZeroPadding2D, UpSampling2D\nfrom keras.layers import Input, Reshape","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:48.690377Z","iopub.execute_input":"2021-12-19T19:10:48.690868Z","iopub.status.idle":"2021-12-19T19:10:48.702272Z","shell.execute_reply.started":"2021-12-19T19:10:48.690818Z","shell.execute_reply":"2021-12-19T19:10:48.701371Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def make_generator_model():\n    model = Sequential()\n\n    model.add(Dense(128*128,activation=\"relu\",input_dim=100)) #128x128 units\n    model.add(Reshape((8,8,256)))\n\n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    \n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    \n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n   \n    model.add(UpSampling2D(size=(2,2)))\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n\n    model.add(UpSampling2D(size=(2,2)))\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n\n    model.add(Conv2D(3,kernel_size=3,padding=\"same\"))\n    model.add(Activation(\"tanh\"))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:48.703803Z","iopub.execute_input":"2021-12-19T19:10:48.704242Z","iopub.status.idle":"2021-12-19T19:10:48.718321Z","shell.execute_reply.started":"2021-12-19T19:10:48.704193Z","shell.execute_reply":"2021-12-19T19:10:48.717198Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[128, 128, 3]))\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(0.2))\n    \n    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:48.735873Z","iopub.execute_input":"2021-12-19T19:10:48.736798Z","iopub.status.idle":"2021-12-19T19:10:48.749149Z","shell.execute_reply.started":"2021-12-19T19:10:48.736752Z","shell.execute_reply":"2021-12-19T19:10:48.748441Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"generator = make_generator_model()\n\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0,:,:,0],interpolation='nearest')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:48.750207Z","iopub.execute_input":"2021-12-19T19:10:48.751317Z","iopub.status.idle":"2021-12-19T19:10:49.655636Z","shell.execute_reply.started":"2021-12-19T19:10:48.751269Z","shell.execute_reply":"2021-12-19T19:10:49.654771Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"discriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint(decision)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:49.658474Z","iopub.execute_input":"2021-12-19T19:10:49.658693Z","iopub.status.idle":"2021-12-19T19:10:49.844843Z","shell.execute_reply.started":"2021-12-19T19:10:49.658666Z","shell.execute_reply":"2021-12-19T19:10:49.843873Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4,0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4,0.5)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:49.846079Z","iopub.execute_input":"2021-12-19T19:10:49.846348Z","iopub.status.idle":"2021-12-19T19:10:49.853642Z","shell.execute_reply.started":"2021-12-19T19:10:49.846320Z","shell.execute_reply":"2021-12-19T19:10:49.852704Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"!rm -rf ./training_checkpoints\n!mkdir ./training_checkpoints\n!rm -rf ./image_at_epoch*\ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:49.855440Z","iopub.execute_input":"2021-12-19T19:10:49.855760Z","iopub.status.idle":"2021-12-19T19:10:52.238731Z","shell.execute_reply.started":"2021-12-19T19:10:49.855704Z","shell.execute_reply":"2021-12-19T19:10:52.237851Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 500\nnoise_dim = 100\nnum_examples_to_generate = 4\n\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:52.240005Z","iopub.execute_input":"2021-12-19T19:10:52.240245Z","iopub.status.idle":"2021-12-19T19:10:52.246423Z","shell.execute_reply.started":"2021-12-19T19:10:52.240219Z","shell.execute_reply":"2021-12-19T19:10:52.245465Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:52.247599Z","iopub.execute_input":"2021-12-19T19:10:52.247932Z","iopub.status.idle":"2021-12-19T19:10:52.260165Z","shell.execute_reply.started":"2021-12-19T19:10:52.247897Z","shell.execute_reply":"2021-12-19T19:10:52.259526Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input):\n  predictions = model(test_input, training=False)\n  predictions = 0.5 * predictions + 0.5\n\n  fig = plt.figure(figsize=(32, 32))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i])\n      plt.axis('off')\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:52.261232Z","iopub.execute_input":"2021-12-19T19:10:52.262085Z","iopub.status.idle":"2021-12-19T19:10:52.276332Z","shell.execute_reply.started":"2021-12-19T19:10:52.262049Z","shell.execute_reply":"2021-12-19T19:10:52.275435Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      t = train_step(image_batch)\n\n    display.clear_output(wait=True)\n    \n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n    # Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    \n\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T19:10:52.277506Z","iopub.execute_input":"2021-12-19T19:10:52.277857Z","iopub.status.idle":"2021-12-19T19:10:52.288927Z","shell.execute_reply.started":"2021-12-19T19:10:52.277826Z","shell.execute_reply":"2021-12-19T19:10:52.287917Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train(train_dataset, 2)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T20:35:35.281112Z","iopub.execute_input":"2021-12-19T20:35:35.281651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Images","metadata":{}},{"cell_type":"code","source":"seed = tf.random.normal([20, noise_dim])\npredictions = generator(seed, training=False)\npredictions = 0.5 * predictions + 0.5\n\nfig = plt.figure(figsize=(10,10))\nfor i in range(1,21):\n    fig.add_subplot(5,5,i)\n    plt.imshow(predictions[i])\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T20:35:20.715767Z","iopub.status.idle":"2021-12-19T20:35:20.716270Z","shell.execute_reply.started":"2021-12-19T20:35:20.716076Z","shell.execute_reply":"2021-12-19T20:35:20.716116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./generated_images\n!mkdir ./generated_images\n\nseed = tf.random.normal([200, noise_dim])\npredictions = generator(seed, training=False)\npredictions = 0.5 * predictions + 0.5\n\nfig = plt.figure(figsize=(10,10))\nfor i in range(1,200):\n    plt.axis('off')\n    plt.imshow(generated_images2[i])\n    plt.savefig('./images/%d.jpg' % i)","metadata":{},"execution_count":null,"outputs":[]}]}